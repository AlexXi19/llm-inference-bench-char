# LLM Inference Benchmarking for Chat 

Gives you the option to use a source prompt at the start, and add some random text after it to control the prompt cache percentage. 

source: https://github.com/leptonai/leptonai/blob/main/misc/benchmark/run.py

